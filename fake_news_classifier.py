# -*- coding: utf-8 -*-
"""Fake News Classifier.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1sepcWTghgo03RulfhGH3GzBiH_-qg1VQ

#Importing Libraries
"""

import numpy as np 
import pandas as pd
import re
import nltk
# from nltk.tokenize import tokenize
from nltk.tokenize import wordpunct_tokenize
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer
from sklearn.ensemble import RandomForestClassifier
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report

"""#Data Gathering"""

news_data = pd.read_csv("/content/drive/MyDrive/train.csv")

"""#Data Analysis"""

news_data.head()

"""The news is classified as:
*   1: *Unreliable*
*   0: *Reliable*




"""

news_data.shape

news_data.info()

news_data['label'].value_counts()

"""The value count of both *Reliable and Unreliable* labels are almost equal so no need of balancing the column."""

news_data.isnull().sum()

"""Handling the null values by dropping them."""

news_data = news_data.dropna()

news_data.isnull().sum()

news_data.shape

news_data.head(10)

"""Resetting the index of the dataframe after dropping the null valued columns."""

news_data.reset_index(inplace = True)

news_data.head()

"""Extracting the required dataset."""

news_data['data'] = news_data['author']+" "+news_data['title']

news_data.head()

news_data = news_data.drop(['id', 'author','text', 'title'] , axis= 1)

news_data.head(10)

"""#Data Preprocessing

The following steps are to be taken:
- Remove symbols
- Convert String to lowercase
- Tokenize the string
- Remove Stopwords
- Lemmatization
- Join the list to a string
- Append in column
"""

import spacy
nlp = spacy.load('en_core_web_sm')

corpus = []

for i in range(len(news_data)):
  content = re.sub('a-zA-Z0-9', ' ', news_data['data'][i]) 
  content = content.lower()
  content = nlp(content)
  content = [token for token in content if not nlp.vocab[token.text].is_stop]
  content = [token.lemma_ for token in content]
  content = " ".join(content)
  corpus.append(content)

len(corpus)

news_data['data'][0]
corpus[0]

"""#Vectorization"""

tf = TfidfVectorizer()
X = tf.fit_transform(corpus).toarray()
X

Y = news_data['label']
Y.head()

"""#Splitting the dataset into training and testing sets


"""

X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.3, random_state = 10, stratify= Y)

len(X_train), len(Y_train)

len(X_test), len(Y_test)

"""#Model Building"""

rf = RandomForestClassifier()
rf.fit(X_train, Y_train)

"""#Model Evaluation"""

Y_pred = rf.predict(X_test)
accuracy_score(Y_test, Y_pred)

class Evaluation:
  def __init__(self, model, x_train, x_test, y_train, y_test):
    self.model = model
    self.x_train = x_train
    self.x_test = x_test
    self.y_train = y_train
    self.y_test = y_test

  def training_evaluation(self):
    y_pred_train = self.model.predict(self.x_train)
    acc_scr_train = accuracy_score(self.y_train, y_pred_train)
    print("The Accuracy Score of the model on testing data is: \n",acc_scr_train, "\n")
    
    con_mat_train = confusion_matrix(self.y_train, y_pred_train)
    print("The Confusion Matrix of the training data prediction is:\n",con_mat_train, "\n")

    class_report = classification_report(self.y_train, y_pred_train , target_names = ["Reliable", "Unreliable"])
    print("The Classification Report on the training data is: \n",class_report)

  def testing_evaluation(self):
    y_pred_test = self.model.predict(self.x_test)
    acc_scr_train = accuracy_score(self.y_test, y_pred_test)
    print("The Accuracy Score of the model on testing data is: \n",acc_scr_train, "\n")
    
    con_mat_train = confusion_matrix(self.y_test, y_pred_test)
    print("The Confusion Matrix of the testing data prediction is:\n",con_mat_train, "\n")

    class_report = classification_report(self.y_test, y_pred_test, target_names = ["Reliable", "Unreliable"])
    print("The Classification Report on the testing data is: \n",class_report)

"""### Chechking the accuracy on training dataset


"""

Evaluation(rf, X_train, X_test, Y_train, Y_test).training_evaluation()

"""### Chechking the accuracy on testing dataset

"""

Evaluation(rf, X_train, X_test, Y_train, Y_test).testing_evaluation()

"""#Prediction Pipeline"""

class Preprocessing:
  
  def __init__(self, data):
    self.data = data

  def text_preprocessing(self):
    prediction_data = [self.data]
    preprocessed_data = []

    for data in prediction_data:
      content = re.sub('a-zA-Z0-9', ' ', data) 
      content = content.lower()
      content = nlp(content)
      content = [token for token in content if not nlp.vocab[token.text].is_stop]
      content = [token.lemma_ for token in content]
      content = " ".join(content)
      preprocessed_data.append(content)
    
    return preprocessed_data

news_data['data'][5]

data = 'Jackie Mason: Hollywood Would Love Trump if He Bombed North Korea over Lack of Trans Bathrooms (Exclusive Video) - Breitbart'
Preprocessing(data).text_preprocessing()

class Prediction:
  
  def __init__(self, prediction_data, model):
    self.prediction_data = prediction_data
    self.model= model

  def prediction(self):
    preprocessed_data = Preprocessing(self.prediction_data).text_preprocessing()
    data_vector = tf.transform(preprocessed_data)
    prediction = self.model.predict(data_vector)

    if prediction[0] == 0:
      print("The news is reliable.")
    else :
      print("The news is fake!")

news_data['data'][5]

data = "Daniel Nussbaum Jackie Mason: Hollywood Would Love Trump if He Bombed North Korea over Lack of Trans Bathrooms (Exclusive Video) - Breitbart"
Prediction(data, rf).prediction()